{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87c4b39",
   "metadata": {},
   "source": [
    "## 1. Datenimport und Initialisierung\n",
    "\n",
    "In diesem Abschnitt werden die OpenJur-Urteilstexte aus dem Datenverzeichnis eingelesen und die technische Datenbasis für die nachfolgenden Verarbeitungsschritte geschaffen. Dazu werden die benötigten Bibliotheken importiert und die verfügbaren Textdateien identifiziert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972de4e7",
   "metadata": {},
   "source": [
    "### 1.1 Import der benötigten Bibliotheken\n",
    "\n",
    "Zu Beginn werden die für die weitere Verarbeitung erforderlichen Python-Bibliotheken importiert. Diese umfassen Funktionen für Dateizugriffe, reguläre Ausdrücke, Datenverarbeitung mit Pandas sowie den Export der Ergebnisse im JSON-Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f86b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a736b",
   "metadata": {},
   "source": [
    "### 1.2 Einlesen der OpenJur-Urteilstexte \n",
    "\n",
    "In diesem Schritt werden alle identifizierten Urteilstexte aus dem Datenverzeichnis eingelesen. Jede Datei wird über den Dateinamen einer eindeutigen Fallkennung (`case_id`) zugeordnet. Die Texte bilden die Rohdatenbasis für die nachfolgenden Extraktions- und Filterprozesse. Der Datenpfad wird im Code parametriert (`DATA_DIR`), um eine reproduzierbare Ausführung zu gewährleisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bef8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pfad: c:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\backend\\data\\Gerichtsurteile_Openjur\n",
      "Anzahl .txt: 2375\n",
      "Erste 10 Dateien: ['2090187.txt', '2112111.txt', '2112115.txt', '2112117.txt', '2112118.txt', '2112119.txt', '2112121.txt', '2112123.txt', '2124977.txt', '2126821.txt']\n"
     ]
    }
   ],
   "source": [
    "# (.txt) Dateien einlesen\n",
    "DATA_DIR = \"../data/Gerichtsurteile_Openjur\" \n",
    "files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "print(\"Pfad:\", os.path.abspath(DATA_DIR))\n",
    "print(\"Anzahl .txt:\", len(files))\n",
    "print(\"Erste 10 Dateien:\", files[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494b356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed94d8a",
   "metadata": {},
   "source": [
    "## 2. Extraktion relevanter Urteilsbestandteile und Selektion der Landgerichtsurteile\n",
    "\n",
    "In diesem Abschnitt werden die eingelesenen Urteilstexte weiterverarbeitet, um für die nachfolgende Analyse relevante Textbestandteile gezielt zu extrahieren. Hierzu zählen insbesondere ein begrenzter Kopfbereich zur Voranalyse sowie der Tenor als Kern der gerichtlichen Entscheidung. Die strukturierte Aufbereitung dieser Textsegmente bildet die Grundlage für Filter-, Klassifikations- und Extraktionsschritte in den folgenden Abschnitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c915ba",
   "metadata": {},
   "source": [
    "### 2.1 Aufbau des DataFrames und Extraktion eines Kopfbereichs\n",
    "\n",
    "Die eingelesenen Texte werden in einem DataFrame (`df`) gespeichert. Zusätzlich wird ein begrenzter Kopfbereich (`head`) aus den ersten Zeichen extrahiert, da strukturelle Metadaten wie Gerichtstyp, Entscheidungsart und Zitierzeilen typischerweise am Anfang des Dokuments auftreten. Dieser Kopfbereich dient als effizienter Suchraum für die spätere Identifikation von Landgerichtsurteilen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28198986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt eingelesen: 2375\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for fn in files:\n",
    "    case_id = fn.replace(\".txt\", \"\")\n",
    "    path = os.path.join(DATA_DIR, fn)\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "    rows.append({\"case_id\": case_id, \"text\": text})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Gesamt eingelesen:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a88600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head-Länge (Beispiel): 8000\n"
     ]
    }
   ],
   "source": [
    "HEAD_CHARS = 8000\n",
    "df[\"head\"] = df[\"text\"].astype(str).str.slice(0, HEAD_CHARS)\n",
    "\n",
    "print(\"Head-Länge (Beispiel):\", len(df.loc[0, \"head\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b62f66",
   "metadata": {},
   "source": [
    "### 2.2 Extraktion des Tenors\n",
    "\n",
    "Der Tenor enthält die eigentliche gerichtliche Entscheidung und ist daher für die inhaltliche Bewertung besonders relevant. Mithilfe regulärer Ausdrücke wird der Textabschnitt zwischen der Überschrift „Tenor“ und den nachfolgenden Abschnitten (z. B. „Tatbestand“ oder „Gründe“) extrahiert und in einer separaten Spalte gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fae0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenor vorhanden: 2362 von 2375\n"
     ]
    }
   ],
   "source": [
    "def extract_tenor(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    m_start = re.search(r\"\\bTenor\\b\", text, flags=re.IGNORECASE)\n",
    "    if not m_start:\n",
    "        return \"\"\n",
    "\n",
    "    start = m_start.end()\n",
    "\n",
    "    # Begrenztes Suchfenster nach dem Tenor (robuster gegen Navigation)\n",
    "    window = text[start:start + 20000]\n",
    "\n",
    "    m_end = re.search(\n",
    "        r\"\\b(Tatbestand|Gründe|Gruende|Entscheidungsgründe|Entscheidungsgruende)\\b\",\n",
    "        window,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    end = start + m_end.start() if m_end else min(len(text), start + 8000)\n",
    "    return text[start:end].strip()\n",
    "df[\"tenor\"] = df[\"text\"].apply(extract_tenor)\n",
    "print(\"Tenor vorhanden:\", (df[\"tenor\"].str.len() > 0).sum(), \"von\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481ec6a",
   "metadata": {},
   "source": [
    "### 2.3 Identifikation von Landgerichtsurteilen (LG)\n",
    "\n",
    "Die Selektion der Landgerichtsurteile erfolgt anhand einer OpenJur-spezifischen Zitierzeile im Kopfbereich (Regex: „Einfach“ gefolgt von „LG“). Auf dieser Grundlage wird eine boolesche Variable erzeugt und der Teilkorpus df_lg gebildet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6ff89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "✅ Echte LG-Urteile (über Zitierzeile): 1189\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wir suchen nach der Zeile, die mit \"Einfach\" beginnt, gefolgt von \"LG\"\n",
    "# Der Regex r\"Einfach\\s*\\n\\s*LG\" stellt sicher, dass LG direkt darunter steht\n",
    "pattern_zitierung_lg = r\"Einfach\\s*\\n\\s*LG\"\n",
    "\n",
    "# Wir wenden das auf die Spalte an, die den Kopftext enthält\n",
    "df[\"is_landgericht\"] = df[\"head\"].str.contains(pattern_zitierung_lg, regex=True, na=False)\n",
    "\n",
    "# Jetzt erstellen wir den sauberen Dataframe\n",
    "df_lg = df[df[\"is_landgericht\"] == True].copy()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"✅ Echte LG-Urteile (über Zitierzeile): {len(df_lg)}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85194c82",
   "metadata": {},
   "source": [
    "### 2.4 Segmentierung der Urteile in juristische Abschnitte\n",
    "Für die spätere Extraktion werden die Urteile in juristisch sinnvolle Teile zerlegt: Rubrum, Tenor, Tatbestand und Entscheidungsgründe. Dadurch kann das Modell gezielt relevante Passagen verarbeiten.\n",
    "Die Segmentierung dient dazu, spätere Analysen gezielt auf entscheidungsrelevante Abschnitte (insb. Tenor und Entscheidungsgründe) zu fokussieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870d2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_judgment(text):\n",
    "    \"\"\"\n",
    "    Teilt ein Urteil in Rubrum, Tenor, Tatbestand und Entscheidungsgründe auf.\n",
    "    \"\"\"\n",
    "    segments = {\n",
    "        \"rubrum\": \"\",\n",
    "        \"tenor\": \"\",\n",
    "        \"tatbestand\": \"\",\n",
    "        \"entscheidungsgruende\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Muster für die Abschnittsüberschriften\n",
    "    # Das Rubrum ist alles vor dem Tenor\n",
    "    m_tenor = re.search(r\"\\bTenor\\b\", text, re.IGNORECASE)\n",
    "    m_tatbestand = re.search(r\"\\bTatbestand\\b\", text, re.IGNORECASE)\n",
    "    m_gruende = re.search(r\"\\b(Entscheidungsgründe|Gründe)\\b\", text, re.IGNORECASE)\n",
    "    \n",
    "    if m_tenor:\n",
    "        segments[\"rubrum\"] = text[:m_tenor.start()].strip()\n",
    "        \n",
    "        # Tenor bis Tatbestand\n",
    "        if m_tatbestand:\n",
    "            segments[\"tenor\"] = text[m_tenor.end():m_tatbestand.start()].strip()\n",
    "            \n",
    "            # Tatbestand bis Gründe\n",
    "            if m_gruende:\n",
    "                segments[\"tatbestand\"] = text[m_tatbestand.end():m_gruende.start()].strip()\n",
    "                segments[\"entscheidungsgruende\"] = text[m_gruende.end():].strip()\n",
    "            else:\n",
    "                segments[\"tatbestand\"] = text[m_tatbestand.end():].strip()\n",
    "        else:\n",
    "            # Falls kein Tatbestand gefunden wird, Tenor bis zum Ende oder Gründen\n",
    "            if m_gruende:\n",
    "                segments[\"tenor\"] = text[m_tenor.end():m_gruende.start()].strip()\n",
    "                segments[\"entscheidungsgruende\"] = text[m_gruende.end():].strip()\n",
    "            else:\n",
    "                segments[\"tenor\"] = text[m_tenor.end():].strip()\n",
    "                \n",
    "    return segments\n",
    "\n",
    "# Beispielanwendung auf den Dataframe\n",
    "df_lg['segments'] = df_lg['text'].apply(split_judgment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78144122",
   "metadata": {},
   "source": [
    "## 3 Prompt-Generierung und Pilotierung der LLM-Extraktion (Gemini Batch)\n",
    "Um API- und Token-Limits zu berücksichtigen, werden OpenJur-spezifische Navigationselemente aus dem Rubrum entfernt und alle Abschnitte in ihrer Länge begrenzt. Auf Basis dieser vorverarbeiteten Textsegmente wird ein standardisierter Prompt generiert, der die Extraktion der abgestimmten Variablen im JSON-Format steuert.\n",
    "Die segmentweise Längenbegrenzung dient der Einhaltung von Token-Limits sowie der Reduktion von Kosten und Laufzeit, ohne entscheidungsrelevante Passagen (insb. Tenor und Entscheidungsgründe) zu verlieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039d706",
   "metadata": {},
   "source": [
    "### 3.1 Aufbereitung der Segmente und Definition des Extraktions-Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "753fa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rubrum(rubrum: str) -> str:\n",
    "    if not isinstance(rubrum, str):\n",
    "        return \"\"\n",
    "\n",
    "    blacklist = [\n",
    "        \"rechtsprechung\", \"aktuell\", \"trending\", \"filter\",\n",
    "        \"über openjur\", \"spenden\", \"api\", \"hilfe\",\n",
    "        \"startseite\", \"bundesland\", \"gerichtsbarkeit\",\n",
    "        \"impressum\", \"datenschutz\", \"nutzungsbedingungen\",\n",
    "        \"fachzeitschriften\", \"suchen\", \"changelog\", \"einfach\",\n",
    "        \"json\", \"bibtex\", \"ris\"\n",
    "    ]\n",
    "\n",
    "    lines = []\n",
    "    for line in rubrum.splitlines():\n",
    "        l = line.strip().lower()\n",
    "        if not l:\n",
    "            continue\n",
    "        if any(b in l for b in blacklist):\n",
    "            continue\n",
    "        lines.append(line.strip())\n",
    "\n",
    "    return \"\\n\".join(lines[:5])   \n",
    "\n",
    "def slim_segments(segments):\n",
    "    return {\n",
    "        \"rubrum\": clean_rubrum(segments.get(\"rubrum\") or \"\")[:2500],\n",
    "        \"tenor\": (segments.get(\"tenor\") or \"\")[:4000],\n",
    "        \"tatbestand\": (segments.get(\"tatbestand\") or \"\")[:3500],\n",
    "        \"entscheidungsgruende\": (segments.get(\"entscheidungsgruende\") or \"\")[:7000],\n",
    "    }\n",
    "\n",
    "def get_gemini_prompt(segments):\n",
    "    \"\"\"\n",
    "    Erstellt den finalen Prompt basierend auf den Urteilssegmenten.\n",
    "    \"\"\"\n",
    "    s = slim_segments(segments)   # <--- DAS ist der entscheidende Schritt\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analysiere die folgenden Abschnitte eines Gerichtsurteils zum Dieselskandal und extrahiere die Variablen präzise als JSON-Liste. \n",
    "\n",
    "### URTEILS-BESTANDTEILE:\n",
    "RUBRUM (Kopfbereich mit Gericht & Datum): \n",
    "{s['rubrum']}\n",
    "\n",
    "TENOR (Ergebnis): \n",
    "{s['tenor']}\n",
    "\n",
    "TATBESTAND (Sachverhalt): \n",
    "{s['tatbestand']}\n",
    "\n",
    "ENTSCHEIDUNGSGRÜNDE (Rechtliche Würdigung): \n",
    "{s['entscheidungsgruende']}\n",
    "\n",
    "### EXTRAKTIONS-AUFGABE:\n",
    "Extrahiere folgende Variablen (bei Nichtfinden 'null' angeben):\n",
    "\n",
    "1. **Input-Variablen (Features):**\n",
    "   - Dieselmotor_Typ\n",
    "   - Art_Abschalteinrichtung\n",
    "   - KBA_Rueckruf\n",
    "   - Fahrzeugstatus\n",
    "   - Fahrzeugmodell_Baureihe\n",
    "   - Update_Status\n",
    "   - Kilometerstand_Kauf\n",
    "   - Kilometerstand_Klageerhebung\n",
    "   - Erwartete_Gesamtlaufleistung\n",
    "   - Kaufdatum\n",
    "   - Uebergabedatum\n",
    "   - Datum_Klageerhebung\n",
    "   - Nachweis_Aufklaerung\n",
    "   - Beklagten_Typ\n",
    "   - Datum_Urteil\n",
    "   - Kaufpreis\n",
    "   - Nacherfuellungsverlangen_Fristsetzung\n",
    "   - Klageziel\n",
    "   - Rechtsgrundlage\n",
    "\n",
    "2. **Zielvariablen (Labels):**\n",
    "   - LABEL_Anspruch_Schadensersatz\n",
    "   - LABEL_Schadensersatzhoehe_Betrag\n",
    "   - LABEL_Schadensersatzhoehe_Range\n",
    "\n",
    "### AUSGABEFORMAT:\n",
    "Antworte NUR mit einem validen JSON-Objekt in einer Liste:\n",
    "[{{\n",
    "  \"case_id\": \"...\",\n",
    "  \"Dieselmotor_Typ\": null,\n",
    "  \"Art_Abschalteinrichtung\": null,\n",
    "  \"KBA_Rueckruf\": null,\n",
    "  \"Fahrzeugstatus\": null,\n",
    "  \"Fahrzeugmodell_Baureihe\": null,\n",
    "  \"Update_Status\": null,\n",
    "  \"Kilometerstand_Kauf\": null,\n",
    "  \"Kilometerstand_Klageerhebung\": null,\n",
    "  \"Erwartete_Gesamtlaufleistung\": null,\n",
    "  \"Kaufdatum\": null,\n",
    "  \"Uebergabedatum\": null,\n",
    "  \"Datum_Klageerhebung\": null,\n",
    "  \"Nachweis_Aufklaerung\": null,\n",
    "  \"Beklagten_Typ\": null,\n",
    "  \"Datum_Urteil\": null,\n",
    "  \"Kaufpreis\": null,\n",
    "  \"Nacherfuellungsverlangen_Fristsetzung\": null,\n",
    "  \"Klageziel\": null,\n",
    "  \"Rechtsgrundlage\": null,\n",
    "  \"LABEL_Anspruch_Schadensersatz\": null,\n",
    "  \"LABEL_Schadensersatzhoehe_Betrag\": null,\n",
    "  \"LABEL_Schadensersatzhoehe_Range\": null\n",
    "}}]\n",
    "\"\"\".strip()\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a84b7b",
   "metadata": {},
   "source": [
    "### 3.2 Erstellung eines Pilot-Inputs im JSONL-Format\n",
    "Zur technischen Validierung der Analysepipeline wird ein Pilotdatensatz erzeugt, der eine begrenzte Anzahl von Landgerichtsurteilen umfasst. Für jedes ausgewählte Urteil werden die zuvor definierten Textsegmente extrahiert, zu einem standardisierten Analyse-Prompt zusammengeführt und im JSONL-Format gespeichert. Diese Pilotdatei dient als Testeingabe für die nachgelagerte Verarbeitung über die Gemini-API, bevor eine Skalierung auf den vollständigen Datensatz erfolgt.\n",
    "Der Pilot dient ausschließlich der technischen Validierung der Prompt-Struktur und der Batch-Pipeline und ist nicht für eine inhaltliche Evaluation der Extraktionsergebnisse vorgesehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f6ac57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pilot erstellt: gemini_batch_input_pilot_10.jsonl\n"
     ]
    }
   ],
   "source": [
    "PILOT_N = 10\n",
    "pilot_path = \"gemini_batch_input_pilot_10.jsonl\"\n",
    "\n",
    "with open(pilot_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df_lg.head(PILOT_N).iterrows():\n",
    "        segments = row[\"segments\"]\n",
    "        full_prompt = get_gemini_prompt(segments)\n",
    "\n",
    "        payload = {\n",
    "            \"custom_id\": f\"case_{row['case_id']}\",\n",
    "            \"contents\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [{\"text\": full_prompt}]\n",
    "            }]\n",
    "        }\n",
    "        f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ Pilot erstellt:\", pilot_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26beec66",
   "metadata": {},
   "source": [
    "Der Code dient der inhaltlichen und technischen Validierung der erzeugten Pilotdatei. Hierzu wird der erste Eintrag der JSONL-Datei geladen und exemplarisch ausgegeben, um Struktur, Inhalt und Länge des generierten Analyse-Prompts zu überprüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8773a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_2090187\n",
      "Analysiere die folgenden Abschnitte eines Gerichtsurteils zum Dieselskandal und extrahiere die Variablen präzise als JSON-Liste. \n",
      "\n",
      "### URTEILS-BESTANDTEILE:\n",
      "RUBRUM (Kopfbereich mit Gericht & Datum): \n",
      "Rechtsgebiet\n",
      "Gericht\n",
      "Informationen\n",
      "\n",
      "TENOR (Ergebnis): \n",
      "I. Die Klage wird abgewiesen.II. Der Kläger hat die Kosten des Rechtsstreits zu tragen.III. Das Urteil ist gegen Sicherheitsleistung in Höhe des 1,1-fachen des zu vollstreckenden Betrags vorläufig vollstreckbar.IV. Der Streitwert wird auf 31.234,00 € festgesetzt.\n",
      "\n",
      "TATBESTAND (Sachverhalt): \n",
      "Der Kläger begehrt Lieferung eines mangelfreien Pkw.Der Kläger erwarb von der Beklagten im Jahr 2014 einen Neuwagen VW Passat 2,0 l TDI für 31.234,00 €. Der Pkw ist von dem \"VW-Abgasskandal\" betroffen. Der Kläger hat die Beklagte im Jahr 2016 durch Anwa\n",
      "Prompt-Länge: 8908\n"
     ]
    }
   ],
   "source": [
    "with open(\"gemini_batch_input_pilot_10.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first = json.loads(f.readline())\n",
    "\n",
    "print(first[\"custom_id\"])\n",
    "print(first[\"contents\"][0][\"parts\"][0][\"text\"][:800])\n",
    "print(\"Prompt-Länge:\", len(first[\"contents\"][0][\"parts\"][0][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966495f0",
   "metadata": {},
   "source": [
    "### 3.3 Upload und Start eines Pilot-Batch-Jobs\n",
    "In diesem Schritt wird die zuvor erzeugte Pilot-JSONL-Datei als Eingabe für die Gemini-API hochgeladen. Die Datei enthält strukturierte Analyseanfragen für mehrere Urteile und wird auf den Servern bereitgestellt, sodass sie anschließend im Rahmen einer Batch- oder sequenziellen Verarbeitung vom Sprachmodell verarbeitet werden kann. Der Upload erzeugt eine referenzierbare Eingabedatei, die anschließend einem eindeutig benannten Batch-Job zugewiesen wird und damit eine reproduzierbare Verarbeitung durch das Sprachmodell ermöglicht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371274c8",
   "metadata": {},
   "source": [
    "Initialisierung des API-Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cb802c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialisiert\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY ist nicht gesetzt\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "print(\"Client initialisiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d462c1",
   "metadata": {},
   "source": [
    "Upload der JSONL-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee753d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload: files/8i63btgg96qz\n"
     ]
    }
   ],
   "source": [
    "uploaded = client.files.upload(\n",
    "    file=\"gemini_batch_input_pilot_10.jsonl\",\n",
    "    config={\n",
    "        \"display_name\": \"diesel-lg-pilot-10\",\n",
    "        \"mime_type\": \"text/plain\"\n",
    "    }\n",
    ")\n",
    "print(\"Upload:\", uploaded.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e7aedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m job = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/gemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43muploaded\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisplay_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdiesel-lg-pilot-10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch gestartet:\u001b[39m\u001b[33m\"\u001b[39m, job.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\batches.py:1965\u001b[39m, in \u001b[36mBatches.create\u001b[39m\u001b[34m(self, model, src, config)\u001b[39m\n\u001b[32m   1963\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(model=model, src=src, config=config)\n\u001b[32m   1964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\batches.py:1552\u001b[39m, in \u001b[36mBatches._create\u001b[39m\u001b[34m(self, model, src, config)\u001b[39m\n\u001b[32m   1549\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   1550\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m1552\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1556\u001b[39m response_dict = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1378\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1381\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1382\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1383\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m   response_body = (\n\u001b[32m   1388\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m   )\n\u001b[32m   1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1192\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m       method=http_request.method,\n\u001b[32m   1194\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\humme\\OneDrive\\Dokumente\\Uni Ulm\\ds_law\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}"
     ]
    }
   ],
   "source": [
    "job = client.batches.create(\n",
    "    model=\"models/gemini-2.5-flash\",\n",
    "    src=uploaded.name,\n",
    "    config={\"display_name\": \"diesel-lg-pilot-10\"}\n",
    ")\n",
    "\n",
    "print(\"Batch gestartet:\", job.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99f72e",
   "metadata": {},
   "source": [
    "## 4 Verarbeitung der Modellantworten und Erstellung des Extraktions-Datensatzes\n",
    "\n",
    "In diesem Abschnitt werden die Batch-Ausgaben der Gemini-API eingelesen, validiert und in ein tabellarisches Format überführt. Zum Zeitpunkt der aktuellen Notebook-Version liegt lediglich der Pilot-Workflow vor; der Code zur Verarbeitung des vollständigen Batch-Outputs wird nach Abschluss des Batch-Jobs ergänzt.\n",
    "\n",
    "(gemini_batch_input_NUR_LG.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaca577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Code einfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b810e",
   "metadata": {},
   "source": [
    "### 4.1 Download/Export der Batch-Ausgabedatei (JSONL) (Platzhalter)\n",
    "\n",
    "Nach der in Abschnitt 2 beschriebenen Aufbereitung der Urteilstexte liegt der vollständige Analyse-Datensatz in Form einer strukturierten JSONL-Datei vor. Diese Datei dient in diesem Schritt als Eingabe für die automatisierte Verarbeitung durch ein großes Sprachmodell.\n",
    "\n",
    "Die JSONL-Datei wird zunächst in das Batch-System hochgeladen. Anschließend wird ein Batch-Verarbeitungsjob gestartet, der die hochgeladene Datei als Eingabequelle verwendet. Für jedes enthaltene Dokument erzeugt das Modell eine strukturierte Antwort gemäß den im Prompt definierten Extraktionsvorgaben.\n",
    "\n",
    "Als Ergebnis des Batch-Jobs stellt die API eine Ausgabedatei bereit, die die Modellantworten zu allen verarbeiteten Urteilen enthält. Diese Ausgabedatei liegt ebenfalls im JSONL-Format vor und bildet die Grundlage für die weitere Aufbereitung und Auswertung der Ergebnisse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f543fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Code einfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7cbd2",
   "metadata": {},
   "source": [
    "### 4.2 Parsing, Validierung und Tabellierung der Extraktionen (Platzhalter)\n",
    "\n",
    "Die im vorherigen Schritt erzeugte Ausgabedatei des Batch-Jobs liegt zunächst als Rohdaten im JSONL-Format vor. Jede Zeile dieser Datei enthält die strukturierte Modellantwort zu einem einzelnen Landgerichtsurteil.\n",
    "\n",
    "Diese Rohdaten werden lokal gespeichert und anschließend in ein tabellarisches Format überführt. Hierzu werden die relevanten Felder aus den JSON-Strukturen extrahiert und in einer einheitlichen Datenstruktur zusammengeführt, beispielsweise in Form einer CSV-Datei. \n",
    "Der so erzeugte Datensatz bildet die Grundlage für die weitere statistische Auswertung und Analyse in den folgenden Abschnitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Code einfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911c4f4",
   "metadata": {},
   "source": [
    "### 4.3 Zusammenführung mit Metadaten und Speicherung (CSV/Parquet) (Platzhalter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c339b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f899d3",
   "metadata": {},
   "source": [
    "## 5. Datenaufbereitung für maschinelles Lernen\n",
    "\n",
    "In diesem Abschnitt werden die Urteilstexte für die nachgelagerte prädiktive Modellierung aufbereitet. Hierzu erfolgt zunächst eine juristisch angepasste Textvorverarbeitung und die Ableitung numerischer Textrepräsentationen. Die für die supervised Lernphase erforderlichen Zielvariablen werden im Rahmen der LLM-basierten Extraktion (Abschnitt 4) erzeugt und anschließend mit den Textmerkmalen zusammengeführt (Abschnitt 5.4).\n",
    "Ziel der Datenaufbereitung ist es, die extrahierten Merkmale in eine konsistente, auswertbare Form zu überführen, fehlende oder uneinheitliche Angaben zu behandeln und die Zielvariablen für die spätere Analyse eindeutig zu definieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ab869",
   "metadata": {},
   "source": [
    "### 5.1 Juristische Textvorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b021aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Setup: Spezialisiertes deutsches Sprachmodell laden\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_lg\", disable=[\"ner\", \"parser\"])\n",
    "except Exception:\n",
    "    print(\"Bitte installiere das spacy Modell: python -m spacy download de_core_news_lg\")\n",
    "\n",
    "# --- 2. JURISTISCHE TEXTVORVERARBEITUNG ---\n",
    "def legal_preprocess(text):\n",
    "    \"\"\"\n",
    "    Bereitet juristische Texte auf, indem Rauschen entfernt wird, \n",
    "    während rechtlich relevante Zahlen und Kontexte geschützt werden.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "\n",
    "    # NEU: START DES URTEILS FINDEN (Rauschschnitt Anfang) ---\n",
    "    # Wir schneiden Webseiten-Menüs (\"trending\", \"suche\" etc.) weg\n",
    "    start_keywords = [\"tenor\", \"entscheidungsgründe\", \"tatbestand\", \"urteil\", \"beschluss\", \"endurteil\"]\n",
    "    text_lower_start = text.lower()\n",
    "    \n",
    "    # Finde die früheste Position eines der Keywords\n",
    "    found_positions = [text_lower_start.find(kw) for kw in start_keywords if text_lower_start.find(kw) != -1]\n",
    "    if found_positions:\n",
    "        text = text[min(found_positions):]\n",
    "\n",
    "    # NEU: ENDE DES URTEILS FINDEN (Rauschschnitt Ende) ---\n",
    "    # Wir schneiden Impressum und Footer weg\n",
    "    end_keywords = [\"impressum\", \"nutzungsbedingungen\", \"nach oben\", \"datenschutz\"]\n",
    "    text_lower_end = text.lower()\n",
    "    for ekw in end_keywords:\n",
    "        e_pos = text_lower_end.find(ekw)\n",
    "        if e_pos != -1:\n",
    "            text = text[:e_pos]\n",
    "            break\n",
    "\n",
    "    # 1. Bereinigung von Rauschen (HTML-Tags, Sonderzeichen)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "    # 2. Schutz von Zahlen & Paragraphen (Platzhalter statt Löschen)\n",
    "    # Euro-Beträge schützen\n",
    "    text = re.sub(r'\\d{1,3}(?:\\.\\d{3})*(?:,\\d+)?\\s*(?:EUR|€|Euro)', ' PLATZHALTER_BETRAG ', text)\n",
    "    # Paragraphen schützen\n",
    "    text = re.sub(r'§+\\s*\\d+[a-z]?\\s*(?:\\w+)?', ' PLATZHALTER_PARAGRAPH ', text)\n",
    "    # Jahreszahlen schützen\n",
    "    text = re.sub(r'\\b(19|20)\\d{2}\\b', ' PLATZHALTER_JAHR ', text)\n",
    "\n",
    "    # 3. Kleinschreibung zur Reduktion der Varianz\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4. Tokenisierung und Lemmatisierung mit SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # 5. Kontextsensitive Stoppwort-Entfernung\n",
    "    # Wichtige juristische Negationen schützen\n",
    "    protected_negations = {\"nicht\", \"kein\", \"ohne\", \"gegen\", \"trotz\"}\n",
    "    custom_stop_words = nlp.Defaults.stop_words - protected_negations\n",
    "    \n",
    "    # Extraktion der Lemmata (Grundformen)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.lemma_ not in custom_stop_words \n",
    "        and not token.is_punct \n",
    "        and not token.is_space\n",
    "        and len(token.text) > 1 # Token mit Länge 1 entfernen\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# --- 2.5 HILFSFUNKTIONEN: Simulation + echtes Batch lesen ---\n",
    "def get_llm_text(r: dict) -> str:\n",
    "    # Simulation (simulated_batch_output.jsonl)\n",
    "    if \"text\" in r:\n",
    "        return r[\"text\"]\n",
    "    # Echtes Batch (später)\n",
    "    if \"response\" in r:\n",
    "        return r[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    raise KeyError(\"Unbekanntes Ergebnisformat (kein 'text' und kein 'response').\")\n",
    "\n",
    "def parse_llm_json(text: str) -> dict:\n",
    "    # Entfernt ```json ... ``` falls vorhanden\n",
    "    text = re.sub(r\"^```json\\s*|\\s*```$\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    # Falls außenrum Text steht: ersten JSON-Block extrahieren\n",
    "    m = re.search(r\"(\\{.*\\})\", text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        text = m.group(1)\n",
    "    return json.loads(text)\n",
    "\n",
    "# --- 3. MERGING DER DATEN (URTEILE + EXTRAKTIONEN) ---\n",
    "def merge_and_finalize(judgment_file, batch_results_file):\n",
    "    \"\"\"\n",
    "    Führt die ursprünglichen Urteilstexte mit den Gemini-Extraktionen zusammen.\n",
    "    \"\"\"\n",
    "    # 1. Laden der aufbereiteten LG-Urteile\n",
    "    df_judgments = pd.read_json(judgment_file, lines=True)\n",
    "    df_judgments['case_id'] = df_judgments['custom_id'].str.replace('case_', '')\n",
    "\n",
    "    # 2. Laden der Gemini-Batch-Ergebnisse\n",
    "    with open(batch_results_file, 'r', encoding='utf-8') as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "    \n",
    "    extracted_rows = []\n",
    "    for r in results:\n",
    "        try:\n",
    "            case_id = r['custom_id'].replace('case_', '')\n",
    "            llm_text = get_llm_text(r)\n",
    "            content = parse_llm_json(llm_text)\n",
    "            content['case_id'] = case_id\n",
    "            extracted_rows.append(content)\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    df_extracted = pd.DataFrame(extracted_rows)\n",
    "\n",
    "    # 3. Zusammenführung über case_id \n",
    "    df_final = pd.merge(df_judgments, df_extracted, on='case_id', how='inner')\n",
    "\n",
    "    # 4. Textverarbeitung anwenden\n",
    "    print(\"Starte Textvorverarbeitung...\")\n",
    "    df_final['cleaned_text'] = df_final['text'].apply(legal_preprocess)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# --- 4. MODELL-VORBEREITUNG (TF-IDF) ---\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),   # Bigramme erhalten Wortzusammenhänge\n",
    "    max_features=1000,    # Reduktion der Komplexität\n",
    "    min_df=5              # Seltene Begriffe ignorieren\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTS_FILE = \"simulated_batch_output.jsonl\"   # später echte Batch-Output-Datei\n",
    "# df_final = merge_and_finalize(\n",
    "    judgment_file=\"lg_judgments.jsonl\",\n",
    "    batch_results_file=RESULTS_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4388ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_final shape: (1189, 8)\n",
      "non-empty cleaned_text: 1185\n",
      "\n",
      "Beispiel cleaned_text:\n",
      "\n",
      "Urteil gegen Sicherheitsleistung Höhe 1,1-fache vollstreckend betrag vorläufig vollstreckbar.iv Streitwert Platzhalter_betrag festsetzen zulässig Klage unbegründet.d Kläger gegen beklagen Anspruch Nachlieferung mangelfrei Pkw aktuell Produktion Platzhalter_paragraph Platzhalter_paragraph Alternative\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# 1) Sicherstellen, dass Segmente existieren\n",
    "if \"segments\" not in df_lg.columns:\n",
    "    df_lg[\"segments\"] = df_lg[\"text\"].apply(split_judgment)\n",
    "\n",
    "df_final = df_lg.copy()\n",
    "\n",
    "# 2) Text für Embeddings: TENOR + ENTSCHEIDUNGSGRÜNDE\n",
    "def build_text_for_embedding(s):\n",
    "    if not isinstance(s, dict):\n",
    "        return \"\"\n",
    "    return (s.get(\"tenor\") or \"\") + \"\\n\" + (s.get(\"entscheidungsgruende\") or \"\")\n",
    "\n",
    "df_final[\"text_for_embedding\"] = df_final[\"segments\"].apply(build_text_for_embedding)\n",
    "\n",
    "# 3) Länge begrenzen (wichtig für Laufzeit)\n",
    "MAX_CHARS = 12000\n",
    "df_final[\"text_for_embedding\"] = (\n",
    "    df_final[\"text_for_embedding\"]\n",
    "    .astype(str)\n",
    "    .str.slice(0, MAX_CHARS)\n",
    ")\n",
    "\n",
    "# 4) Juristisches Preprocessing (einmal, mit Fortschritt)\n",
    "df_final[\"cleaned_text\"] = df_final[\"text_for_embedding\"].apply(legal_preprocess)\n",
    "\n",
    "# 5) Sanity-Checks\n",
    "print(\"df_final shape:\", df_final.shape)\n",
    "print(\n",
    "    \"non-empty cleaned_text:\",\n",
    "    (df_final[\"cleaned_text\"].str.len() > 0).sum()\n",
    ")\n",
    "\n",
    "# Vorschau\n",
    "print(\"\\nBeispiel cleaned_text:\\n\")\n",
    "print(df_final[\"cleaned_text\"].iloc[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3b2a0",
   "metadata": {},
   "source": [
    "### 5.2 Semantische Repräsentation der Urteilstexte mittels Word Embeddings (Skip-Gram)\n",
    "Zur semantischen Repräsentation der Landgerichtsurteile wurden die Textsegmente Tenor und Entscheidungsgründe herangezogen und einer juristisch angepassten Textvorverarbeitung unterzogen. Auf dieser Grundlage wurde ein Word2Vec-Modell in der Skip-Gram-Variante trainiert, um kontextabhängige Wortrepräsentationen zu erzeugen. Die Vektordimension wurde auf 200 festgelegt, das Kontextfenster auf acht Wörter begrenzt und eine minimale Wortfrequenz von fünf definiert. Zur Abbildung ganzer Urteile wurden die Wortvektoren eines Dokuments durch Mittelwertbildung zu einer festen, 200-dimensionalen Dokumentenrepräsentation aggregiert. Die resultierenden Embeddings bilden die Grundlage für den in Abschnitt 5.3 beschriebenen Analyse-Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "398a5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisierte Texte für Word2Vec\n",
    "sentences = [str(t).split() for t in df_final[\"cleaned_text\"].dropna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a77edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-Gram Word2Vec Modell trainieren\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=200,   # Dimension der Wortvektoren\n",
    "    window=8,          # Kontextfenster\n",
    "    min_count=5,       # sehr seltene Wörter ignorieren\n",
    "    workers=4,\n",
    "    sg=1,              # <-- Skip-Gram (besser für Fachbegriffe, prüfen) \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a5aa75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabulargröße: 10137\n"
     ]
    }
   ],
   "source": [
    "print(\"Vokabulargröße:\", len(w2v_model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34222237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dokumenten-Vektor durch Mittelung der Wortvektoren\n",
    "import numpy as np\n",
    "\n",
    "def document_vector(doc, model):\n",
    "    if not isinstance(doc, str) or not doc.strip():\n",
    "        return np.zeros(model.vector_size)\n",
    "    words = doc.split()\n",
    "    vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Alle Urteile vektorisieren\n",
    "X_embeddings = np.vstack(\n",
    "    df_final[\"cleaned_text\"].apply(lambda x: document_vector(x, w2v_model))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bee56",
   "metadata": {},
   "source": [
    "### 5.3 Aufbau des Analyse-Datensatzes\n",
    "Auf Basis der in Abschnitt 5.2 erzeugten Dokumenten-Embeddings (basierend auf Tenor und Entscheidungsgründen) wird im Folgenden ein strukturierter Analyse-Datensatz aufgebaut. Ziel dieses Schrittes ist die Zusammenführung der semantischen Repräsentationen in eine einheitliche Feature-Matrix, die als Grundlage für die nachgelagerte prädiktive Modellierung dient. Der Analyse-Datensatz ist modellunabhängig konzipiert und ermöglicht eine spätere Erweiterung um Zielvariablen aus der automatisierten Extraktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30ae8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate case_id: 0\n",
      "df_features shape: (1189, 201)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_190</th>\n",
       "      <th>emb_191</th>\n",
       "      <th>emb_192</th>\n",
       "      <th>emb_193</th>\n",
       "      <th>emb_194</th>\n",
       "      <th>emb_195</th>\n",
       "      <th>emb_196</th>\n",
       "      <th>emb_197</th>\n",
       "      <th>emb_198</th>\n",
       "      <th>emb_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2090187</td>\n",
       "      <td>-0.004762</td>\n",
       "      <td>-0.086754</td>\n",
       "      <td>0.010609</td>\n",
       "      <td>-0.017486</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.055343</td>\n",
       "      <td>0.070078</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>-0.142745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206452</td>\n",
       "      <td>-0.076481</td>\n",
       "      <td>-0.140407</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>0.088505</td>\n",
       "      <td>-0.045014</td>\n",
       "      <td>-0.294006</td>\n",
       "      <td>-0.000782</td>\n",
       "      <td>-0.044469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2112111</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>-0.060710</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>0.116090</td>\n",
       "      <td>-0.052467</td>\n",
       "      <td>0.039605</td>\n",
       "      <td>0.198002</td>\n",
       "      <td>-0.092647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202453</td>\n",
       "      <td>-0.107988</td>\n",
       "      <td>-0.166004</td>\n",
       "      <td>-0.128510</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.105017</td>\n",
       "      <td>-0.028844</td>\n",
       "      <td>-0.266960</td>\n",
       "      <td>-0.014790</td>\n",
       "      <td>-0.028805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2112115</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>-0.085715</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.024143</td>\n",
       "      <td>0.167288</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.248859</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150049</td>\n",
       "      <td>-0.061851</td>\n",
       "      <td>-0.181581</td>\n",
       "      <td>-0.179724</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.086053</td>\n",
       "      <td>-0.008226</td>\n",
       "      <td>-0.214873</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>-0.071257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2112117</td>\n",
       "      <td>-0.002766</td>\n",
       "      <td>-0.085585</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>-0.020082</td>\n",
       "      <td>0.155980</td>\n",
       "      <td>-0.009324</td>\n",
       "      <td>0.074384</td>\n",
       "      <td>0.190882</td>\n",
       "      <td>-0.078383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186634</td>\n",
       "      <td>-0.062496</td>\n",
       "      <td>-0.141939</td>\n",
       "      <td>-0.171081</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.101324</td>\n",
       "      <td>-0.053650</td>\n",
       "      <td>-0.259515</td>\n",
       "      <td>0.022684</td>\n",
       "      <td>-0.071714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2112118</td>\n",
       "      <td>0.073236</td>\n",
       "      <td>-0.040248</td>\n",
       "      <td>0.050639</td>\n",
       "      <td>-0.042828</td>\n",
       "      <td>0.163705</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.236122</td>\n",
       "      <td>-0.097973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160654</td>\n",
       "      <td>-0.066206</td>\n",
       "      <td>-0.186293</td>\n",
       "      <td>-0.182968</td>\n",
       "      <td>-0.024537</td>\n",
       "      <td>0.122129</td>\n",
       "      <td>-0.058649</td>\n",
       "      <td>-0.248000</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>-0.042093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id     emb_0     emb_1     emb_2     emb_3     emb_4     emb_5  \\\n",
       "0  2090187 -0.004762 -0.086754  0.010609 -0.017486  0.019369  0.055343   \n",
       "1  2112111  0.010132 -0.060710  0.041788 -0.002412  0.116090 -0.052467   \n",
       "2  2112115  0.013954 -0.085715  0.027826  0.024143  0.167288  0.037509   \n",
       "3  2112117 -0.002766 -0.085585  0.027017 -0.020082  0.155980 -0.009324   \n",
       "4  2112118  0.073236 -0.040248  0.050639 -0.042828  0.163705  0.005957   \n",
       "\n",
       "      emb_6     emb_7     emb_8  ...   emb_190   emb_191   emb_192   emb_193  \\\n",
       "0  0.070078  0.211299 -0.142745  ...  0.206452 -0.076481 -0.140407 -0.156326   \n",
       "1  0.039605  0.198002 -0.092647  ...  0.202453 -0.107988 -0.166004 -0.128510   \n",
       "2  0.076233  0.248859 -0.109954  ...  0.150049 -0.061851 -0.181581 -0.179724   \n",
       "3  0.074384  0.190882 -0.078383  ...  0.186634 -0.062496 -0.141939 -0.171081   \n",
       "4  0.101124  0.236122 -0.097973  ...  0.160654 -0.066206 -0.186293 -0.182968   \n",
       "\n",
       "    emb_194   emb_195   emb_196   emb_197   emb_198   emb_199  \n",
       "0 -0.008802  0.088505 -0.045014 -0.294006 -0.000782 -0.044469  \n",
       "1  0.017313  0.105017 -0.028844 -0.266960 -0.014790 -0.028805  \n",
       "2  0.009528  0.086053 -0.008226 -0.214873  0.010926 -0.071257  \n",
       "3  0.000569  0.101324 -0.053650 -0.259515  0.022684 -0.071714  \n",
       "4 -0.024537  0.122129 -0.058649 -0.248000  0.005803 -0.042093  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aufbau des Analyse-Datensatzes\n",
    "\n",
    "# Feature-Namen für die Embeddings\n",
    "emb_cols = [f\"emb_{i}\" for i in range(X_embeddings.shape[1])]\n",
    "\n",
    "# Embeddings als DataFrame\n",
    "df_features = pd.DataFrame(X_embeddings, columns=emb_cols)\n",
    "\n",
    "# case_id ergänzen (für spätere Joins mit Labels)\n",
    "df_features.insert(0, \"case_id\", df_final[\"case_id\"].reset_index(drop=True).values)\n",
    "\n",
    "# optional: Duplikate prüfen (sollte 0 sein)\n",
    "print(\"Duplicate case_id:\", df_features[\"case_id\"].duplicated().sum())\n",
    "\n",
    "print(\"df_features shape:\", df_features.shape)\n",
    "df_features[[\"case_id\"] + [c for c in df_features.columns if c.startswith(\"emb_\")]].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ca574",
   "metadata": {},
   "source": [
    "Der Analyse-Datensatz besteht aus 1.189 Beobachtungen (Urteilen) mit jeweils 200 numerischen Merkmalen, die den semantischen Gehalt der Entscheidungsgründe abbilden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457d4a6",
   "metadata": {},
   "source": [
    "### 5.4 Modellierung und Evaluation (nach Verfügbarkeit der Labels)\n",
    "Auf Grundlage des in Abschnitt 5.3 aufgebauten Analyse-Datensatzes erfolgt im Folgenden die prädiktive Modellierung. Hierzu werden die semantischen Dokumenten-Embeddings mit den aus der automatisierten Extraktion gewonnenen Zielvariablen verknüpft und für den Einsatz überwachter Lernverfahren vorbereitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b18330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4 (wird aktiviert sobald df_labels aus Batch da ist) ---\n",
    "\n",
    "# df_ml = df_features.merge(df_labels, on=\"case_id\", how=\"inner\")\n",
    "# X = df_ml.filter(like=\"emb_\").values\n",
    "# y = df_ml[\"LABEL_Anspruch_Schadensersatz\"].astype(int).values\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1, class_weight=\"balanced\")\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# y_pred = rf.predict(X_test)\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05160c8f",
   "metadata": {},
   "source": [
    "## 6. Analyse und Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49984b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
